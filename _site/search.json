[
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Adam Peterson",
    "section": "",
    "text": "Attribution 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More considerations\n for the public: \nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "",
    "text": "After writing up my notes on Thomas Lumley’s excellent book detailing how to use his survey methods software package and some of the associated theory, I found myself wanting to cover the subject further. In particular, I found myself reading Lumley’s main theoretical reference for his book, (Särndal, Swensson, and Wretman 2003), and enjoying it immensely.\nConsequently this post will be the first in a series breaking down more advanced topics in design based inference drawing from (Särndal, Swensson, and Wretman 2003) and other sources. In particular, where my notes from Lumley’s text focused more on how to analyze samples, this post will focus on different ways to draw samples.\n\n\nIn this post I’ll provide an overview of several different types of sampling designs one might encounter in design based inference. The inferential goal to start will simply be descriptive, i.e. what is the mean or total for some variable in a population, though its worth noting causal inference and experimental design share many of the same motivations and ideas.\nWhile sampling designs are defined by the probability distribution of drawing any given sample, the implementation of any algorithm that produces the desired sampling distribution is another matter that can have implications on how easily one can draw a sample in practice — this is especially true in the world of big data.\nDetails here become complicated depending on what’s being measured and the infrastructure in place determining how easily samples can be drawn and/or measured. I’ll address some of these issues and refer to my own experience in working in this area to highlight what I think are the best strategies."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#sampling-design",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#sampling-design",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "",
    "text": "In this post I’ll provide an overview of several different types of sampling designs one might encounter in design based inference. The inferential goal to start will simply be descriptive, i.e. what is the mean or total for some variable in a population, though its worth noting causal inference and experimental design share many of the same motivations and ideas.\nWhile sampling designs are defined by the probability distribution of drawing any given sample, the implementation of any algorithm that produces the desired sampling distribution is another matter that can have implications on how easily one can draw a sample in practice — this is especially true in the world of big data.\nDetails here become complicated depending on what’s being measured and the infrastructure in place determining how easily samples can be drawn and/or measured. I’ll address some of these issues and refer to my own experience in working in this area to highlight what I think are the best strategies."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#sec-notation",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#sec-notation",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "Notation",
    "text": "Notation\nI’ll assume that every element in the population can be drawn into the sample and subsequently measured. Indexing each element as i=1,...,N, so that there are N total elements in the population, the measurement on each subject is y_i which can have any support - discrete or continuous. The typical target of estimation is the population total, T = \\sum_{i=1}^{N} y_i, or mean \\mu = \\frac{T}{N}.\nThe sampling design defines how we sample the elements of the population. This structure is in part encoded by their sampling probabilities, \\pi_i, which are defined for each element. The sampling probability represents, as its name implies, the probability that the element is included in the sample, \\mathrm{S}, from the population; P(i \\in \\mathrm{S}) = \\pi_i:\nThe \\pi estimator, is constructed by weighting each measured element in \\mathrm{S}, where |\\mathrm{S}| = n &lt; N, according to the inverse of its sampling probability.\n\n\\hat{T}_{\\pi} = \\sum_{i=1}^{n} \\frac{y_i}{\\pi_i}.\n\nBy weighting the measurements, we can ensure that the resulting estimate is unbiased for the true population total. The proof is straightforward and can be found here.\nSimilarly the variance has the following general form, \n\\hat{V}[T_{\\pi}] = \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\frac{y_i y_j}{\\pi_{ij}} - \\frac{y_i}{\\pi_i}\\frac{y_j}{\\pi_j},\n\nwhere \\pi_{ij} is the probability that the ith and jth elements are sampled together. Again, a proof is found here in question 10.\n\nNote that a different formulation of this estimator is used for designs that sample with replacement.\n\nAs I work through the different designs it’ll become clear that the way in which we structure the sampling can have a profound impact on our ability to efficiently estimate any population quantity. In particular, depending on how the \\pi_i are constructed from the sampling design, our variance expression shown above can change for the better or worse."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#bernoulli-sampling",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#bernoulli-sampling",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "Bernoulli Sampling",
    "text": "Bernoulli Sampling\nA Bernoulli sample is arguably the simplest sampling design. Iterating through each element of the population, flip a coin that shows heads with some specified probability. If the coin shows heads, include the sample in your population, if not, don’t include it. The same principle can be applied by using a random uniform number generator and applying a cutoff according to the same principle. The probability of flipping a heads, \\pi is the sampling probability for all elements. Since each indicator is independent of the other, the joint sampling probability is simply the product, \\pi_{ij} = \\pi_i^2, i \\neq j.\nThis algorithm is straightforward and can be implemented in a variety of ways, if the population can be stored in memory in a simple vector, the function below shows one way to draw a Bernoulli sample.\n\nbernoulli_sample &lt;- function(population_data, pi) {\n  N &lt;- length(population_data)\n  return(population_data[runif(n = N) &lt;= pi])\n}\n\nAlternatively, a call to dplyr::slice_sample() on a data.frame will perform the same operation.\n\nmtcars |&gt;\n  slice_sample(prop = .1)\n\n              mpg cyl  disp  hp drat   wt qsec vs am gear carb\nMerc 240D    24.4   4 146.7  62 3.69 3.19 20.0  1  0    4    2\nMerc 230     22.8   4 140.8  95 3.92 3.15 22.9  1  0    4    2\nFerrari Dino 19.7   6 145.0 175 3.62 2.77 15.5  0  1    5    6\n\n\nNote that the inclusion probability independence property here — and elsewhere — will allow this sampling design to be easily parallelized if the entire population’s information is stored on disk or sharded.\nHowever, this advantage comes at a cost. Since the sample indicators are all independent, the size of the sample is also random. This can be undesirable for administrative purposes and also causes the \\pi estimator for this design to be less efficient than the straightforward sample mean \\bar{y} — scaled by N if estimating the population total. This greater efficiency comes from adjusting the estimate according to the observed sample size, rather than relying on the random sample size information from the sampling probabilities.\n\nThe alternative estimator uses the observed sample size, n, as opposed to the expected sample size, E[n] = N \\pi.\n\nIn my work with large sharded datasets, I often find myself using the Bernoulli sample to quickly from large data sets when I’m trying to get a quick sense of what the data looks like as it is so quick to run. As long as the variable of interest isn’t rare and measurement isn’t expensive, this can be a very effective sample design.\n\nProperties\nFor a Bernoulli sample with a single inclusion probability the \\pi estimator of the population total is \\hat{T} = \\frac{1}{\\pi} \\sum_{i=1}^{n} y_i.\nWith variance estimated by \n\\hat{V}[\\hat{T}_{\\pi}] = \\frac{1}{\\pi}\\left (\\frac{1}{\\pi} - 1 \\right)\\sum_{i=1}^{n} y_i^2.\n\nThe more efficient estimator’s variance estimator is\n\n\\hat{V}[\\hat{T}_{a}] = \\frac{1 - \\pi}{n} \\frac{1}{n-1} \\sum_{i=1}^{n} (y-\\bar{y})^2,\n which follows from classic iid sample variance estimate.\n(Särndal, Swensson, and Wretman 2003) derives the ratio of the two estimators variance to show the latter is more efficient than the former. You can see a plot below showing a comparison of the two estimators estimating the total of the mpg column of the mtcars data set. The alternative estimator is much more efficient than the \\pi estimator.\n\n\nShow the code\ntibble(\n  pi = seq(from = 0.01, to = 1, by = 0.1)\n) |&gt; \n  mutate(\n    y = map(pi, function(pi_) mtcars |&gt; slice_sample(prop = pi_) |&gt; pull(mpg))\n  ) |&gt; \n  group_by(pi) |&gt; \n  unnest(y) |&gt; \n  summarize(\n    # min function here simply takes one of the values since we have duplicates\n    # within each pi group \n    var_pi = min(1 / pi * (1 / pi - 1) * (sum(y^2))),\n    var_alt = min((1 - pi) / n() * var(y) * (var(y)))\n  ) |&gt;\n  gather(contains(\"var\"), key = \"Estimator\", value = \"Variance\") |&gt;\n  mutate(Estimator = if_else(Estimator == \"var_pi\", \"pi\", \"alt\")) |&gt;\n  ggplot(aes(x = pi, y = Variance, color = Estimator)) +\n  geom_point() +\n  geom_line() +\n  theme(legend.title = element_blank()) +\n  xlab(\"Sampling Probability\") +\n  ylab(\"Estimator Variance\") +\n  ggtitle(\"Estimator Variance Comparison\", \n          subtitle = \"Bernoulli Sample Design\")"
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#simple-random-sampling",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#simple-random-sampling",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nA simple random sample (SRS) is likely the most common sample design.\nThe design is defined by setting equal probability on all samples of fixed size n. From this it follows that the first order inclusion probabilities are \\pi_i = \\frac{n}{N} and \\pi_{ij} = \\frac{n(n-1)}{N(N-1)} \\quad k \\neq l = 1. Note that the \\pi_{ij} \\neq \\pi_i \\pi_j because of the fact that we’re sampling without replacement. However, if n is small relative to N \\pi_{ij} in this design will approach its independent counterpart.\nOne simple algorithm for drawing a SRS is as follows:\n\nDraw \\epsilon_1,...,\\epsilon_N \\stackrel{iid}{\\sim} Uniform(0,1).\nIf \\epsilon_1 &lt; \\frac{n}{N}, i=1 is included in the sample, otherwise not.\nFor i=2,...,N let n_i be the number of elements selected among the first i-1 elements in the list.\nInclude element i if \\epsilon_i &lt; \\frac{n - n_i}{N - i + 1}\nTerminate when n elements are selected\n\nThis same algorithm is shown below in R, but can also be utilized by calling the sample() or slice_sample() function.\n\nsimple_random_sample &lt;- function(N, n) {\n  random_indicators &lt;- runif(N)\n  if (random_indicators[1] &lt; n / N) {\n    sample_index_container &lt;- c(1)\n  } else {\n    sample_index_container &lt;- vector()\n  }\n  n_i &lt;- length(sample_index_container)\n  for (i in 2:N) {\n    prob_frac &lt;- n - n_i / (N - i + 1)\n    if (random_indicators[i] &lt; prob_frac) {\n      sample_index_container &lt;- c(sample_index_container, i)\n    }\n  }\n  return(sample_index_container)\n}\n\n\nmtcars |&gt;\n  slice_sample(n = 7)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nAMC Javelin    15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nCamaro Z28     13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n\n\nThis is one of a class of sampling algorithms called “Reservoir samplers” of which you can read more about, both in theoretical and implementation details from wikipedia here. I also enjoyed Florian Hartmann’s blog post on reservoir sampling with streaming data here based on (Vitter 1985).\nNote that in contrast to the Bernoulli sample design, here the sample size is fixed by design, so the \\pi estimator and scaled sample mean are equivalent.\n\nProperties\n\n\\hat{T}_{\\pi} = N\\bar{y} = \\frac{N}{n}\\sum_{i=1}^{n} y_i\n\nwith variance, \n\\hat{V}_{SRS}[\\hat{T}_{\\pi}] = N \\frac{N - n}{n} \\frac{1}{n-1}\\sum_{i=1}^{n}(y_i - \\bar{y})^2\n The population mean can be estimated with the sample mean, and the variance is the same as before but now divided by the population size scaling factor; \\hat{V}_{SRS}[\\hat{\\bar{y}}] = \\frac{N-n}{n} \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^{2}\n\n\nConsiderations\nIn my work I often prefer a fixed sample size when I know that the cost of measurement on the sampled units will be expensive and / or there are serious memory constraints to consider. I generally balance these considerations against how long it takes to draw the sample, as reservoir samplers are typically more time intensive with \\mathcal{O}(n) computational complexity.\n\nRead more about Big O notation here."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#systematic-sampling",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#systematic-sampling",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nA systematic sample offers a huge advantage in terms of drawing the sample. As (Särndal, Swensson, and Wretman 2003) describes the sampling procedure:\n\nA first element is drawn at random, and with equal probability, among the first a elements in the population list. The positive integer a is fixed in advance and is called the sampling interval. No further draw is needed. The rest of the sample is determined by systematically taking every ath element thereafter until the end of the list.\n\nOne simple algorithm to conduct this sample could be written as follows:\n\nsystematic_sample &lt;- function(N, n, a) {\n  random_start &lt;- sample(1:a, 1)\n  sample_indices &lt;- seq(from = random_start, to = N, by = a)\n  # or alternatively\n  # sample_indices &lt;- seq(from = random_start, to = N, length.out = n)\n  return(sample_indices)\n}\n\nIf sampling via SQL interface, a Modulus operator could be used in conjunction with the ROW_NUMBER() function. There’s an analogous implementation via tidyverse functions.\n\n# a is chosen prior to sampling\na &lt;- 5\nrandom_start &lt;- sample(1:a, size = 1, replace = FALSE)\nmtcars |&gt;\n  filter(row_number() %% random_start == 0)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nThis approach leads to straightforward determination of \\pi_i and \\pi_{ij}. Since, under this paradigm, each sample is non-overlapping, \\pi_i = \\frac{1}{a} \\quad \\forall i. Similarly, \n\\pi_{ij} =\n\\begin{cases} \\frac{1}{a} & i \\text{ and } j \\in \\mathrm{S} \\\\ 0 & \\text{o.w.}\n\\end{cases}\n\nNote that this implies that \\pi_{ij} \\ngeq 0 \\quad \\forall i, j. Which requires a separate derivation and consideration of the variance from the formula previously given. The total estimate is still the same - \\hat{T}_{\\pi} = a \\sum_{i=1}^{n} y_i\n\nVariance Difficulties\n(Särndal, Swensson, and Wretman 2003) goes into great detail elaborating on the conditions under which systematic sampling is more or less efficient than a comparable simple random sample.\nThe key property is how the data are ordered - if neighboring variables are very similar to each other, then taking a systematic sample will encourage distinct values are included in the sample and the variability across repeated sampling will be lower than a comparable simple random sample. In spite of this potential systematic sampling offers further difficulty in that no unbiased estimator of the variance is available. (Särndal, Swensson, and Wretman 2003) offers various ways in which one might bound the variability of the estimator if certain details about the data are known prior to sampling but it’ll suffice for my purposes to say that systematic sampling, while easy to implement, has difficult estimation properties.\n\nOther Methods for Handling Systematic Variance\nWhile (Särndal, Swensson, and Wretman 2003) finishes their discussion of the systematic variance there, it is briefly worth noting that this method is still of practical relevance, as the US census employs it in their American Community Survey. In order to accurately compute the variance of their methods they use successive difference replicate weights, one of a class of replicate weights on which you can find more discussion in my notes from Lumley’s book. The interested reader should also look into the svrep package, if they want to make successive difference replicate weights.\n\n\n\nConsiderations\nI’ve never used a systematic sampling design, as it has always been easier for me to simply draw a sample. However, I can see the appeal and may use it in the future given its ease to draw and the — relatively recent — ability to correctly estimate the variance via successive difference weights."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#poisson-sampling",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#poisson-sampling",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "Poisson Sampling",
    "text": "Poisson Sampling\nPoisson sampling is the first design discussed thus far that specifically allows for unequal probability of sampling. In notation, we designate the sampling probability for the ith element as \\pi_i, i = 1,...,N. These sampling probabilities are determined by some method prior to drawing the sample, which I’ll discuss further in the next section. As a natural generalization of the Bernoulli sampling design, Poisson sampling similarly draws elements independently from the population, resulting in \\pi_{ij} = \\pi_i \\pi_j.\nSimilar to Bernoulli sampling, the sample size is random, with an average value equal to the sum of the sampling probabilities and variance equal to the sum of Bernoulli variances:\n\nE[n_s] = \\sum_{i=1}^{N} \\pi_i \\\\\nV[n_s] = \\sum_{i=1}^{N} \\pi_i(1 - \\pi_i).\n The \\pi estimator has the usual form and the variance can be estimated via\n\nV[\\hat{T}_{Poisson}] =  \\sum_{i=1}^{n} (1 - \\pi_i)y_i^2.\n\n(Särndal, Swensson, and Wretman 2003) warns that the variance above can be quite high because of the variability in the sample size. Similar to the Bernoulli sampling design this is the main disadvantage of an independent sampling scheme. However, its simplicity is appealing and if a variable sample size is not problematic, the poisson sample design can be effective.\nNow that there are element specific sampling probabilities slice_sample’s algorithm will not weight the elements appropriately when drawing the samples.\nIn contrast, the sampling R package uses the following simple implementation - where pik is a vector of inclusion probabilities and runif() draws random variables from a Uniform(0,1) distribution.\n\nsampling::UPpoisson\n\nfunction (pik) \n{\n    if (any(is.na(pik))) \n        stop(\"there are missing values in the pik vector\")\n    as.numeric(runif(length(pik)) &lt; pik)\n}\n&lt;bytecode: 0x12e3e1588&gt;\n&lt;environment: namespace:sampling&gt;\n\n\n\nProperties\nThe \\pi estimator works here exactly as intended, but the variance estimator is \n\\hat{V}_{\\pi}[T] = \\sum_{i=1}^{n}(1-\\pi_i)\\frac{y_i^2}{\\pi_i},\n\nwhich can result in a high variance. Similar to the Bernoulli sampling design an alternative estimator, and associated variance estimator can be employed using the expected sample size:\n\n\\hat{T}_{alt} = \\frac{N}{\\hat{N}} \\sum_{i=1}^{n} \\frac{y_i}{\\pi_i}, \\\\\n\\hat{N} = \\sum_{i=1}^{n} \\frac{1}{\\pi_i}.\n\nWhich is a simpler case of a regression estimator for the mean. (Särndal, Swensson, and Wretman 2003) gives the approximate variance for this but no estimator for the variance. I haven’t used a poisson design in this setting before but I’m guessing I would either use the variance estimator from the regression model implemented in the survey R package, or the straightforward HTE variance, since the alternative isn’t given.\n\n\nConsiderations\nAlthough less efficient because of its variable sample size, the Poisson sampling design does highlight how a more efficient design could be constructed. Indeed, (Särndal, Swensson, and Wretman 2003) shows that if an expected fixed sample size is used where \\pi_i \\propto y_i then \\hat{T}_{\\pi} = nc, for some constant c, meaning that the the variance of the estimate of \\hat{T}_{\\pi} would zero. In reality, the \\pi_i are constructed before the y_i are measured, and so constructing them to be perfectly proportional is not feasible. However, following the principle that the more proportional the \\pi_i are to y_i, the lower the variance of the estimate is still very useful. Consequently, this principle motivates the next design."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#pps-sampling",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#pps-sampling",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "PPS Sampling",
    "text": "PPS Sampling\nProbability Proportional-to-Size Sampling or PPS Sampling builds off of the insight from Poisson sampling, that sampling with values proportional to y will lead to a lower variance in the HTE estimator.\nIn practice, one can’t use the y values themselves to draw the sample, as they are obviously the measurement of focus for the survey itself. However, it is often the case that some auxiliary variable, x, may be correlated with y which can be used instead to draw the sample.\nStill, further complications remain, as drawing a sample that satisfies the properties required for the \\pi_i to be truly proportional to the x_k in, say, a fixed-size design without replacement can be quite complicated. For this sampling design then, I’ll list a few broad ideas alongside references for the reader interested learning more and then move to some examples.\n\n(Särndal, Swensson, and Wretman 2003) cites (Sunter 1977)’s list-sequential scheme that is ofen found to be “good enough” for drawing proportional samples.\n\nIt appears that (Tillé and Matei 2023), the sampling R package uses this same algorithm in their inclusionprobabilities() function.\n\n\nAfter computing the inclusion probabilities comes the actual sampling itself.\n\nAgain, the sampling R package has several functions that can be used of which I typically use the UPbrewer() function, though there are others too with various pros/cons for computing and using second order inclusion probabilities.\nWhen sampling in a “big data” setting I’ve used (Cohen et al. 2009)’s method with great success. A simple implementation in go can be found here which can then be implemented in various other parallel data streaming software libraries like Apache’s beam, for example.\n\nHere’s a brief example of how to draw a weighted sample from my ComplexSurvey notes using the sampling package functions described above. Below that, I plot the variable of interest for each of the elements in the population alongside their respective sampling probabilities to show how they relate to one another.\n\npopulation &lt;- tibble(\n  id = 1:1E3,\n  Income = 5E4 * rlnorm(1E3, meanlog = 0, sdlog = 0.5)\n)\n\nweighted_sample &lt;- population %&gt;%\n  mutate(\n    pi = sampling::inclusionprobabilities(floor(population$Income), 50),\n    in_weighted_sample = sampling::UPbrewer(pi) == 1,\n  )\n\nweighted_sample |&gt;\n  filter(Income &gt; 0) |&gt;\n  ggplot(aes(x = Income, y = pi)) +\n  geom_point() +\n  ylab(expression(pi)) +\n  ggtitle(\"Income vs. Sampling Probability\")\n\n\n\n\nThis relationship is exactly as we expect, the sampling probabilities are proportional to the income variable along the line shown.\n\nProperties\nThis is the setting in which the \\pi estimator provides optimal properties. The estimator is unbiased and the variance is expected to be low or zero if we somehow were able to construct the \\pi_i to be exactly proportional to y_i; the estimators are the same as listed in Section 2.1."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#stratified-sampling",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#stratified-sampling",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nThe benefits of stratified sampling have already been covered in my notes on Lumley’s text here. To recap briefly, stratified samples gain their power from identifying some categorical measure that is associated with the outcome of interest. By ensuring that a sufficient number of elements are sampled within this group, a better estimate of the overall population value can be obtained. Additionally, if the strata represent some domain of interest - race, income group or other area of interest, stratified sampling can ensure that the measurement within that domain attains the desired estimate precision.\nStratified sampling is easily combined with any of the sampling designs described previously. One can draw a SRS within strata, or a PPS sample, etc. The key principle is that each strata contains an unbiased estimate of the measure of interest, with associated variance which is then added to the other independent stratified samples.\nHere’s a simple example of how to draw a stratified sample with SRS using tidyverse functions, though functions from the sampling R package can also be used here.\n\nmtcars |&gt;\n  group_by(cyl) |&gt;\n  slice_sample(n = 3)\n\n# A tibble: 9 × 11\n# Groups:   cyl [3]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n2  26       4 120.     91  4.43  2.14  16.7     0     1     5     2\n3  24.4     4 147.     62  3.69  3.19  20       1     0     4     2\n4  18.1     6 225     105  2.76  3.46  20.2     1     0     3     1\n5  19.7     6 145     175  3.62  2.77  15.5     0     1     5     6\n6  17.8     6 168.    123  3.92  3.44  18.9     1     0     4     4\n7  15.5     8 318     150  2.76  3.52  16.9     0     0     3     2\n8  13.3     8 350     245  3.73  3.84  15.4     0     0     3     4\n9  15       8 301     335  3.54  3.57  14.6     0     1     5     8\n\n\n\nConsiderations\nI use stratified sampling regularly. It can be used in a pinch as a heuristic for PPS or to ensure that you’ll attain decent estimate precision in what would otherwise be a sparse area of the population. Considerate use of stratified sampling is critical for an applied statistician in my opinion.\nWhen there are explicit costs associated with measuring sampled units, there are a variety of formulas that will attempt to provide optimal precision for the least cost. These formulas are in my other notes I’ve linked above."
  },
  {
    "objectID": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#what-sample-designs-do-and-do-not-offer",
    "href": "posts/2025-02-15-Sampling-Designs-And-Algorithms-Review/index.html#what-sample-designs-do-and-do-not-offer",
    "title": "Sampling Designs and Algorithms: A Review",
    "section": "What Sample Designs do and do not offer",
    "text": "What Sample Designs do and do not offer\nThis post covered a wide array of sampling designs and their various advantages and disadvantages when drawing a sample from a finite population. There’s still more that can be said here in terms of how elements are drawn from the population or subsequently measured. I’ll turn my attention to some of the issues in the latter category soon."
  },
  {
    "objectID": "posts/2021-05-05-PhD-Lessons-Learned/index.html",
    "href": "posts/2021-05-05-PhD-Lessons-Learned/index.html",
    "title": "PhD Lessons Learned",
    "section": "",
    "text": "I’m offering the following key observations on things I learned in my PhD as a way of documenting for myself some of my major lessons learned. It is also my hope that others may find it useful as a kind of reference list from which they might learn, as I have greatly benefited from other PhD students or professor’s documentation of similar references and tools."
  },
  {
    "objectID": "posts/2021-05-05-PhD-Lessons-Learned/index.html#personal-attitude",
    "href": "posts/2021-05-05-PhD-Lessons-Learned/index.html#personal-attitude",
    "title": "PhD Lessons Learned",
    "section": "(1) Personal Attitude",
    "text": "(1) Personal Attitude\nOne’s personal attitude towards the PhD will define everything else learned from it. One key lesson worth learning early on is to let go of your ego. This is easier said than done, as everyone in academia care’s a lot about their intelligence – obviously. That’s how we got to be in graduate school after all – so learning to accept that you’re not that smart and that there’s still a lot one has to learn before becoming a peer of the academy can take some getting used to. Further, it helps to be acclimated to the lifestyle. You need to be very self-driven to get a PhD, as there is much less structure as compared to a M.S. program and one’s experience is largely determined by what the student makes of it. This means that while, yes you can get up any time you want in the day, it helps if you’re self-disciplined enough to get up at such a time that you still get the things done that you need to in order to make progress in your research."
  },
  {
    "objectID": "posts/2021-05-05-PhD-Lessons-Learned/index.html#approach-to-the-problem-research-philosophy",
    "href": "posts/2021-05-05-PhD-Lessons-Learned/index.html#approach-to-the-problem-research-philosophy",
    "title": "PhD Lessons Learned",
    "section": "(2) Approach to the Problem; Research Philosophy",
    "text": "(2) Approach to the Problem; Research Philosophy\nThere’s two things to think about under this heading: (i) The Science (referring to a specific branch of science) and (ii) the Statistics\n\nThis is where a statistician would typically collaborate with “subject-matter experts”. That is, other scientists who are more familiar with the conceptual models, standards in the field and so on that define the current state of knowledge and research within the domain of study. Questions in this domain may follow fairly easily from the questions that are most often posed in the literature, or they may be more subtly hidden.\nConcerns research that extends old or develops new statistical ideas for the purposes of providing statisticians or other scientists with more tools to model natural phenomenon. For example creating a new distribution on the space of some species of graphs I’d consider to be something like “Pure” Statistics research. It may be the case (for the better) that this is developed with an application in mind and so the statistical idea may be merely an adaptation or tweaking of previously established methods to a new kind of data structure or problem. Both would fall under the heading of what’s called “Methods Research”."
  },
  {
    "objectID": "posts/2021-05-05-PhD-Lessons-Learned/index.html#statistical-philosophy",
    "href": "posts/2021-05-05-PhD-Lessons-Learned/index.html#statistical-philosophy",
    "title": "PhD Lessons Learned",
    "section": "(3) Statistical Philosophy",
    "text": "(3) Statistical Philosophy\nMy foundational training was in what is commonly referred to as “classical” or Frequentist statistics. There’s good reason for this as this approach laid the groundwork for much of the applied statistical work that is done today. From the get-go though, I was interested in and hearing about the “new kid on the block”: Bayesian statistics. Of course, this philosophy isn’t all that new, with the first documented work describing Bayesian statistics, going way back to the 1700s, with a work authored by Laplace. I found there to be a number of reasons Bayesian statistics is worth learning in the modern era, principal among them the phenomenon of “concentration of measure”, which provides evidence that the posterior mode (the frequentist’s standard maximum likelihood estimator) is not an “appropriate” description of the posterior distribution in high dimensional spaces. As high dimensional spaces characterize a lot of the work in advanced statistics, this is of increasing importance. There is a nice case study by Bob Carpenter, that tries to provide intuition for these ideas via simulation, as well as a more technical discussion of these ideas here in the stan forums."
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "2023",
    "section": "",
    "text": "2023\n\nAdam T. Peterson. Veronica J. Berrocal. Emma V. Sanchez-Vaznaugh. Brisa N. Sánchez. “How close and how much? Linking health outcomes to built environment spatial distributions.” Ann. Appl. Stat. 17 (2) 1641 - 1662, June 2023. doi\n\n\n\n2022\n\nJingjing Li, Amy H. Auchincloss, Jana A. Hirsch, Steven J. Melly, Kari A. Moore, Adam Peterson, Brisa N. Sánchez, Exploring the spatial scale effects of built environments on transport walking: Multi-Ethnic Study of Atherosclerosis, Health & Place, Volume 73, 2022, 102722, ISSN 1353-8292, doi.\nLi, J., Peterson, A., Auchincloss, A.H. et al. Comparing effects of Euclidean buffers and network buffers on associations between built environment and transport walking: the Multi-Ethnic Study of Atherosclerosis. Int J Health Geogr 21, 12 (2022). doi\nMassey PM, Kearney MD, Rideau A, Peterson A, Gipson JD, Nianogo RA, Bornstein M, Prelip ML, Glik DC. Measuring impact of storyline engagement on health knowledge, attitudes, and norms: A digital evaluation of an online health-focused serial drama in West Africa. J Glob Health. 2022 May 14;12:04039. doi: 10.7189/jogh.12.04039. PMID: 35567587; PMCID: PMC9107188."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adam Peterson",
    "section": "",
    "text": "I am a Data Scientist at Google. I received my PhD in Biostatistics from the University of Michigan in 2021.\nI write about statistical ideas here, and ideas related to the built environment at my blog xstreetvalidated.\nYou can get updated when I write anywhere via my substack.\n\n\n\n\n\nComplex Survey Notes | Notes on Thomas Lumley’s book and software. | 2024\nrsstap | Spline models for spatial temporal aggregated predictor models | 2020"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Adam Peterson",
    "section": "",
    "text": "I am a Data Scientist at Google. I received my PhD in Biostatistics from the University of Michigan in 2021.\nI write about statistical ideas here, and ideas related to the built environment at my blog xstreetvalidated.\nYou can get updated when I write anywhere via my substack."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Adam Peterson",
    "section": "",
    "text": "Complex Survey Notes | Notes on Thomas Lumley’s book and software. | 2024\nrsstap | Spline models for spatial temporal aggregated predictor models | 2020"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "Sampling Designs and Algorithms: A Review\n\n\n\n\n\nAn overview of different sampling designs\n\n\n\n\n\n\nApr 30, 2025\n\n\nAdam Peterson\n\n\n\n\n\n\n  \n\n\n\n\nPhD Lessons Learned\n\n\n\n\n\nA few lessons learned along the way in completing my PhD\n\n\n\n\n\n\nMay 5, 2021\n\n\nAdam Peterson\n\n\n\n\n\n\nNo matching items"
  }
]